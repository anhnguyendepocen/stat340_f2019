---
title: "Ensemble Methods for Classification"
subtitle: "Majority Vote and Stacking"
output:
  pdf_document:
    keep_tex: true
geometry: margin=1.5cm
header-includes:
   - \usepackage{booktabs}
   - \usepackage{multicol}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Ensembles Example: Ionosphere radar data

This example is adapted from a discussion at https://burakhimmetoglu.com/2016/12/01/stacking-models-for-improved-predictions/

Our data set for today is published and described at https://archive.ics.uci.edu/ml/datasets/ionosphere:

> This radar data was collected by a system in Goose Bay, Labrador. This system consists of a phased array of 16 high-frequency antennas with a total transmitted power on the order of 6.4 kilowatts. See the paper for more details. The targets were free electrons in the ionosphere. "Good" radar returns are those showing evidence of some type of structure in the ionosphere. "Bad" returns are those that do not; their signals pass through the ionosphere. 
> 
> Received signals were processed using an autocorrelation function whose arguments are the time of a pulse and the pulse number. There were 17 pulse numbers for the Goose Bay system. Instances in this databse are described by 2 attributes per pulse number, corresponding to the complex values returned by the function resulting from the complex electromagnetic signal.
> 
> Attribute Information:
> 
> * All 34 are continuous 
> * The 35th attribute is either "good" or "bad" according to the definition summarized above. This is a binary classification task. 

```{r, message = FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(purrr)
library(glmnet)
library(caret)

# read in data
ionosphere <- read_csv("http://www.evanlray.com/data/UCIML/ionosphere/ionosphere.data", col_names = FALSE)

# X2 was all 0's
ionosphere <- ionosphere %>% select(-X2)

# Convert prediction target to a factor
ionosphere$X35 <- factor(ionosphere$X35)


## Initial train/test split ("estimation"/test) and cross-validation folds
set.seed(63770)
tt_inds <- caret::createDataPartition(ionosphere$X35, p = 0.8)
ionosphere_train <- ionosphere %>% slice(tt_inds[[1]])
ionosphere_test <- ionosphere %>% slice(-tt_inds[[1]])

crossval_val_fold_inds <- caret::createFolds(
  y = ionosphere_train$X35, # response variable as a vector
  k = 10 # number of folds for cross-validation
)

get_complementary_inds <- function(x) {
  return(seq_len(nrow(ionosphere_train))[-x])
}
crossval_train_fold_inds <- map(crossval_val_fold_inds, get_complementary_inds)
```

\newpage

### Individual Methods

#### Logistic Regression

```{r, warning=FALSE}
logistic_fit <- train(
  form = X35 ~ .,
  data = ionosphere_train,
  family = "binomial", # this is an argument to glm
  method = "glm", # method for fit
  trControl = trainControl(method = "cv", # evaluate method performance via cross-validation
    number = 10, # number of folds for cross-validation
    index = crossval_train_fold_inds, # I'm specifying which folds to use, for consistency across methods
    indexOut = crossval_val_fold_inds, # I'm specifying which folds to use, for consistency across methods
    returnResamp = "all", # return information from cross-validation
    savePredictions = TRUE, # return validation set predictions from cross-validation
    classProbs = TRUE) # return validation set predicted class probabilities from cross-validation
)

logistic_fit$results

head(logistic_fit$pred)

logistic_fit$pred %>%
  group_by(Resample) %>%
  summarize(accuracy = mean(pred == obs)) %>%
  ungroup() %>%
  summarize(accuracy = mean(accuracy))
```

\newpage

#### KNN

```{r}
knn_fit <- train(
  form = X35 ~ .,
  data = ionosphere_train,
  method = "knn",
  preProcess = "scale",
  trControl = trainControl(method = "cv",
    number = 10,
    index = crossval_train_fold_inds, # I'm specifying which folds to use, for consistency across methods
    indexOut = crossval_val_fold_inds, # I'm specifying which folds to use, for consistency across methods
    returnResamp = "all",
    savePredictions = TRUE,
    classProbs = TRUE),
  tuneGrid = data.frame(k = 1:20)
)

knn_fit$results
```

\newpage

#### Trees

```{r}
rpart_fit <- train(
  form = X35 ~ .,
  data = ionosphere_train,
  method = "rpart",
  trControl = trainControl(method = "cv",
    number = 10,
    index = crossval_train_fold_inds, # I'm specifying which folds to use, for consistency across methods
    indexOut = crossval_val_fold_inds, # I'm specifying which folds to use, for consistency across methods
    returnResamp = "all",
    savePredictions = TRUE,
    classProbs = TRUE),
  tuneGrid = data.frame(cp = seq(from = 0, to = 1, length = 20))
)

rpart_fit$results
```

#### Test set predictions from each of the 3 methods above:

```{r}
logistic_preds <- predict(logistic_fit, newdata = ionosphere_test)
mean(ionosphere_test$X35 != logistic_preds)

knn_preds <- predict(knn_fit, newdata = ionosphere_test)
mean(ionosphere_test$X35 != knn_preds)

rpart_preds <- predict(rpart_fit, newdata = ionosphere_test)
mean(ionosphere_test$X35 != rpart_preds)
```

\newpage

### Ensemble Methods

#### Majority Vote

```{r}
majority_vote_preds <- ifelse(
  (logistic_preds == "g") + (knn_preds == "g") + (rpart_preds == "g") >= 2,
  "g",
  "b"
)

mean(ionosphere_test$X35 != majority_vote_preds)
```

#### Mean of Class Probabilities from Individual Methods

```{r}
logistic_class_probs <- predict(logistic_fit, newdata = ionosphere_test, type = "prob")
logistic_prob_g <- logistic_class_probs[, 2]

knn_class_probs <- predict(knn_fit, newdata = ionosphere_test, type = "prob")
knn_prob_g <- knn_class_probs[, 2]

rpart_class_probs <- predict(rpart_fit, newdata = ionosphere_test, type = "prob")
rpart_prob_g <- rpart_class_probs[, 2]

mean_prob_g <- (logistic_prob_g + knn_prob_g + rpart_prob_g) / 3
mean_prob_preds <- ifelse(
  mean_prob_g >= 0.5,
  "g",
  "b"
)
mean(ionosphere_test$X35 != mean_prob_preds)
```

\newpage

#### Stacking: Fit a model to combine predicted class membership probabilities

**Process:**

Estimation: 

1. Get cross-validated predictions for each "stage 1" or "component" model (this was done above)
2. Create a new data set where the explanatory variables are the cross-validated predictions from the component models
3. Fit a "stage 2" model to predict the response based on the component model predictions

Prediction for test set:

4. Re-fit each component model to the full training data set, make predictions for the test set from each component model (this was done above)
5. Create a new data set where the explanatory variables are the test set predictions from the component models
6. Predict using the stage 2 model fit from step 3 and the data frame created in step 5.

```{r, warning=FALSE}
# Step 2: data set with component model predictions as explanatory variables
ionosphere_train <- ionosphere_train %>%
  mutate(
    logistic_prob_g = logistic_fit$pred %>%
      arrange(rowIndex) %>%
      pull(g),
    knn_prob_g = knn_fit$pred %>%
      filter(k == 1) %>%
      arrange(rowIndex) %>%
      pull(g),
    rpart_prob_g = rpart_fit$pred %>%
      filter(cp == rpart_fit$bestTune$cp) %>%
      arrange(rowIndex) %>%
      pull(g)
  )

# Step 3: fit model using component model predictions as explanatory variables
stacking_logistic_fit <- train(
  form = X35 ~ logistic_prob_g + knn_prob_g + rpart_prob_g,
  data = ionosphere_train,
  family = "binomial",
  method = "glm"
)

# Step 5: Assemble data frame of test set predictions from each component model
stacking_test_x <- data.frame(
  logistic_prob_g = logistic_prob_g,
  knn_prob_g = knn_prob_g,
  rpart_prob_g = rpart_prob_g
)

# Step 6: Stacked model predictions
stacking_preds <- predict(stacking_logistic_fit, stacking_test_x)

# Calculate error rate
mean(ionosphere_test$X35 != stacking_preds)
```

\newpage

#### Stacking via KNN

 * We could also use other methods for the second stage model.

```{r}
ionosphere_train <- ionosphere_train %>%
  mutate(
    logistic_prob_g = logistic_fit$pred %>%
      arrange(rowIndex) %>%
      pull(g),
    knn_prob_g = knn_fit$pred %>%
      filter(k == 1) %>%
      arrange(rowIndex) %>%
      pull(g),
    rpart_prob_g = rpart_fit$pred %>%
      filter(cp == rpart_fit$bestTune$cp) %>%
      arrange(rowIndex) %>%
      pull(g)
  )

stacking_knn_fit <- train(
  form = X35 ~ logistic_prob_g + knn_prob_g + rpart_prob_g,
  data = ionosphere_train,
  method = "knn"
)

# Assemble data frame of test set predictions from each
# component model (these were obtained in the previous part)
stacking_test_x <- data.frame(
  logistic_prob_g = logistic_prob_g,
  knn_prob_g = knn_prob_g,
  rpart_prob_g = rpart_prob_g
)

# Stacked model predictions
stacking_preds <- predict(stacking_knn_fit, stacking_test_x)
mean(ionosphere_test$X35 != stacking_preds)
```

### Notes about relative performance of these methods

 * The results with the seed I have set make the stacking approaches look amazing - but in different runs I did as I was developing this, relative performance of the ensemble approaches here varied.
 * In general, stacking is the best of these ensemble approaches in terms of expected value of performance, if the methods' performance is not all equal and we have enough training set data.
 * Note that in this example we had only 3 stage 1/component models.  In general, ensembles are most useful when we have:
    * A large number of models that are "diverse"/uncorrelated/predictions are close to independent
    * Enough training data available to reliably tell which component models are best and how to combine their predictions effectively.
 * Formally, the stacking procedure I did here is not quite right:
    * You shouldn't use the same cross-validation results both to select tuning parameters like K for KNN and cp for classification trees, AND as inputs to stacking.
    * Doing this means models that overfit validation data will get too much weight
    * In practice if you're selecting one tuning parameter this shouldn't matter too much.
